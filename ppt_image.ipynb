{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spire.presentation.common import *\n",
    "from spire.presentation import *\n",
    "\n",
    "# Create a Presentation object\n",
    "presentation = Presentation()\n",
    "\n",
    "# Load a PowerPoint presentation\n",
    "presentation.LoadFromFile(\"Project Delivery.pptx\")\n",
    "\n",
    "# Create the Output directory if it doesn't exist\n",
    "output_directory = \"Output\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Loop through the slides in the presentation\n",
    "for i, slide in enumerate(presentation.Slides):\n",
    "    # Specify the output file name\n",
    "    fileName = f\"{output_directory}/ToImage_{i}.png\"  # Use forward slashes\n",
    "    print(\"Saving slide\", i, \"to\", fileName)  # Print debug information\n",
    "    # Save each slide as a PNG image\n",
    "    image = slide.SaveAsImage()\n",
    "    image.Save(fileName)\n",
    "    image.Dispose()\n",
    "\n",
    "presentation.Dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "image_elements = []\n",
    "output_path = \"C:\\\\Users\\\\DELL\\\\PDF_Chat_MM\\\\Output\"\n",
    "\n",
    "# Function to encode images\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return encoded_image\n",
    "\n",
    "for image_file in os.listdir(output_path):\n",
    "    if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(output_path, image_file)\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_elements.append(encoded_image)\n",
    "print(len(image_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import openai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "chain_gpt = ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=1024)\n",
    "chain_gemini_vision = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\",max_output_tokens=1024)\n",
    "\n",
    "# Function for image content\n",
    "def content_image(encoded_image):\n",
    "    prompt = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"Extract full information in the image.If there is table you need to read it as a table.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    response = chain_gemini_vision.invoke([prompt])\n",
    "    return response.content\n",
    "\n",
    "# Function for image summaries\n",
    "def summarize_image(encoded_image):\n",
    "    prompt = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the contents of this image.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    response = chain_gemini_vision.invoke([prompt])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing image elements with feedback and sleep\n",
    "image_summaries = []\n",
    "image_content=[]\n",
    "for i, ie in enumerate(image_elements):\n",
    "    summary = summarize_image(ie)\n",
    "    image_summaries.append(summary)\n",
    "    contents = content_image(ie)\n",
    "    image_content.append(contents)\n",
    "\n",
    "    print(f\"{i + 1}th element of images processed.\")\n",
    "    print(summary)\n",
    "\n",
    "    print(f\"{i + 1}th element of images processed.\")\n",
    "    print(contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the vector store and storage layer\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key)\n",
    "\n",
    "# Function to add documents to the retriever\n",
    "def add_documents_to_retriever(summaries, original_contents):\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, original_contents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add image summaries\n",
    "add_documents_to_retriever(image_summaries, image_content) # hopefully real images soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can retrieve this table\n",
    "retriever.get_relevant_documents(\n",
    "    \"what is the global market size in asia in 2020?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text, images and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "     \"\"\" what is the global market size in asia in 2020?\n",
    "     use the following information to answer the question:\n",
    "[' # Global Market Size\\nThe image shows the global market size from 2015 to 2023. The market size is expected to grow from 8 billion USD in 2015 to 15 billion in 2023. The Americas region is expected to remain the largest market, followed by the Asia region.\\n\\n## Market Size Global ( in Billion USD)\\n| Year | Americas | EMEA | Asia | Far East | Total |\\n|---|---|---|---|---|---|\\n| 2015 | 4.0 | 1.8 | 0.8 | 1.4 | 8.0 |\\n| 2016 | 4.8 | 2.1 | 1.7 | 1.2 | 9.5 |\\n| 2017 | 5.3 | 1.9 | 2.3 | 1.1 | 10.5 |\\n| 2018 | 6.0 | 2.2 | 2.6 | 1.2 | 12.0 |\\n| 2019 | 7.0 | 2.5 | 3.1 | 1.4 | 14.0 |\\n| 2020 | 6.5 | 2.3 | 2.9 | 1.3 | 13.0 |\\n| 2021 | 6.0 | 2.2 | 2.6 | 1.2 | 12.0 |\\n| 2022 | 6.8 | 2.2 | 3.0 | 1.4 | 13.5 |\\n| 2023 | 7.5 | 2.3 | 3.3 | 1.5 | 15.0 |',\n",
    " ' # Annual Revenue\\n\\n| Year | Americas | EMEA | Asia | Far East | Total |\\n|---|---|---|---|---|---|\\n| 2015 | 1.29 | 0.52 | 0.52 | 0.26 | 2.58 |\\n| 2016 | 1.54 | 0.61 | 0.61 | 0.31 | 3.07 |\\n| 2017 | 1.79 | 0.72 | 0.72 | 0.36 | 3.58 |\\n| 2018 | 1.98 | 0.79 | 0.79 | 0.40 | 3.97 |\\n| 2019 | 2.18 | 0.87 | 0.87 | 0.44 | 4.35 |\\n| 2020 | 2.27 | 0.91 | 0.91 | 0.45 | 4.54 |\\n| 2021 | 2.09 | 0.84 | 0.84 | 0.42 | 4.18 |\\n| 2022 | 2.35 | 0.94 | 0.94 | 0.47 | 4.69 |\\n| 2023 | 2.56 | 1.02 | 1.02 | 0.51 | 5.11 |',\n",
    " ' Project Delivery Team\\n1. Program Director/Delivery Manager\\n2. Program Manager\\n3. Enterprise Architect/Solution Architects\\n4. Team Leads\\n5. Developers/Testers',\n",
    " ' SDLC Waterfall Framework\\n\\n1. Project Kick off\\n2. Requirements Analysis\\n3. Design\\n4. HLD\\n5. LLD\\n6. Development & Testing\\n7. Deployment\\n8. Maintenance']   \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
